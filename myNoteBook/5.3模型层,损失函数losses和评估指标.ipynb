{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.模型层layers\n",
    "\n",
    "深度学习一般由各种模型层组合而来，tf.keras.layers内置非常丰富的模型层，比如：layers.Dense, layers.Flatten, layers.Input, layers.DenseFeature, layers.Dropout, layers.Conv2D, layers.MaxPooling2D, layers.Conv1D, layers.Embedding, layers.GRU, layers.LSTM, layers.Bidirectional等等．\n",
    "\n",
    "还可以通过编写tf.keras.Lambda匿名模型层(只用于构造没有学习参数的模型层)或者继承tf.keras.layers.Layer基类构建自定义的模型层．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １．内置模型层\n",
    "\n",
    "**基础层**\n",
    "\n",
    "* Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)\n",
    "\n",
    "* Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。\n",
    "\n",
    "* Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。\n",
    "\n",
    "* BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。\n",
    "\n",
    "* SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。\n",
    "\n",
    "* Input：输入层。通常使用Functional API方式构建模型时作为第一层。\n",
    "\n",
    "* DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。\n",
    "\n",
    "* Flatten：压平层，用于将多维张量压成一维。\n",
    "\n",
    "* Reshape：形状重塑层，改变输入张量的形状。\n",
    "\n",
    "* Concatenate：拼接层，将多个张量在某个维度上拼接。\n",
    "\n",
    "* Add：加法层。\n",
    "\n",
    "* Subtract： 减法层。\n",
    "\n",
    "* Maximum：取最大值层。\n",
    "\n",
    "* Minimum：取最小值层。\n",
    "\n",
    "**卷积网络相关层**\n",
    "\n",
    "* Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数\n",
    "\n",
    "* Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数\n",
    "\n",
    "* Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数\n",
    "\n",
    "* SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。\n",
    "\n",
    "* DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。\n",
    "\n",
    "* Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。\n",
    "\n",
    "* LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。\n",
    "\n",
    "* MaxPooling2D: 二维最大池化层。也称作下采样层。池化层无参数，主要作用是降维。\n",
    "\n",
    "* AveragePooling2D: 二维平均池化层。\n",
    "\n",
    "* GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。\n",
    "\n",
    "* GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。\n",
    "\n",
    "**循环网络相关层**\n",
    "\n",
    "* Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。\n",
    "\n",
    "* LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。\n",
    "\n",
    "* GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。\n",
    "\n",
    "* SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。\n",
    "\n",
    "* ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。\n",
    "\n",
    "* Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。\n",
    "\n",
    "* RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。\n",
    "\n",
    "* LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。\n",
    "\n",
    "* GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。\n",
    "\n",
    "* Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.自定义模型层\n",
    "\n",
    "如果自定义模型层没有需要被训练的参数，一般推荐使用Lambda层实现，如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现．\n",
    "\n",
    "Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单．\n",
    "\n",
    "Lambda层的正向逻辑可以使用Python的lambda函数来表达，特可以使用def关键字定义函数来表达．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,regularizers\n",
    "\n",
    "mypower = layers.Lambda(lambda x:tf.math.pow(x,2))\n",
    "mypower(tf.range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer的子类化一般需要重现实现初始化方法，Build和Call方法，下为简化的线性层的范例，类似Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32,**kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    ## build方法一般定义Layer需要被训练的参数\n",
    "    def build(self,input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer='random_normal',trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal',trainable=True)\n",
    "        super(Linear,self).build(input_shape) ## 相当于设置了self.built=True\n",
    "    \n",
    "    ## call方法一般定义正向传播运算逻辑，__call＿方法调用\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    ## 如果要让自定义的Layer通过Functional　API 组合成模型时可以序列化，需要自定义get_config方法\n",
    "    def get_config(self):\n",
    "        config = super(Linear,self).get_config()\n",
    "        config.update({'units':self.units})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units=8)\n",
    "print(linear.built)\n",
    "##指定input_shape,显式调用build方法，第0维代表样本数量，用None填充\n",
    "linear.build(input_shape=(None,16))\n",
    "print(linear.built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8)\n"
     ]
    }
   ],
   "source": [
    "print(linear.compute_output_shape(input_shape=(None,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "{'name': 'linear_4', 'trainable': True, 'dtype': 'float32', 'units': 16}\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units=16)\n",
    "print(linear.built)\n",
    "## 如果built＝False，调用__call__方法时会先调用build方法，再调用call方法\n",
    "linear(tf.random.uniform((100,64)))\n",
    "print(linear.built)\n",
    "config = linear.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_shape:  (None, 64)\n",
      "model.output_shape: (None, 16)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "linear (Linear)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 1,040\n",
      "Trainable params: 1,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##直接在模型层使用\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "##注意此处的input_shape会被模型处理，无需使用None代表样本数据维\n",
    "model.add(Linear(units=16,input_shape=(64,)))\n",
    "print('model.input_shape: ', model.input_shape)\n",
    "print('model.output_shape:', model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.损失函数losses\n",
    "\n",
    "监督学习的目标函数由损失函数和正则化项组成(Objective = Loss + Regularization）\n",
    "\n",
    "对于keras模型，目标函数的正则化项一般在各层中指定，例如使用Dense的kernel_regularizer和bias_regularizer等参数指定权重使用l1和l2正则化项，此外还可以使用kernel_constraint和bias_constraint等参数约束权重的取值范围，这也是一种正则化手段.\n",
    "\n",
    "损失函数在模型编译时指定，对于回归模型，一般使用的损失函数为mean_squared_error\n",
    "\n",
    "对于二分类，一般使用二元交叉熵binary_crossentropy.\n",
    "\n",
    "对于多元类，如果label为one-hot编码，可以使用类别交叉熵损失函数categorical_crossentropy.如果label为类别序号编码，则需要使用稀疏类别交叉熵损失函数sparse_categorical_crossentropy.\n",
    "\n",
    "如果需要，还可以自定义损失函数，需要接受两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数数值．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,regularizers,constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １．损失函数和正则化项示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 4,810\n",
      "Trainable params: 4,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(64,input_dim=64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01),\n",
    "                      kernel_constraint=constraints.MaxNorm(max_value=2,axis=0)\n",
    "                      ))\n",
    "model.add(layers.Dense(10,kernel_regularizer=regularizers.l1_l2(0.01,0.01),activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',metrics=['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.内置的损失函数\n",
    "\n",
    "内置的损失函数一般有类的实现和函数的实现两种形式。\n",
    "\n",
    "如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。\n",
    "\n",
    "常用的一些内置损失函数说明如下。\n",
    "\n",
    "* mean_squared_error（平方差误差损失，用于回归，简写为 mse, 类实现形式为 MeanSquaredError 和 MSE）\n",
    "\n",
    "* mean_absolute_error (绝对值误差损失，用于回归，简写为 mae, 类实现形式为 MeanAbsoluteError 和 MAE)\n",
    "\n",
    "* mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类实现形式为 MeanAbsolutePercentageError 和 MAPE)\n",
    "\n",
    "* Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)\n",
    "\n",
    "* binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)\n",
    "\n",
    "* categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)\n",
    "\n",
    "* sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)\n",
    "\n",
    "* hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)\n",
    "\n",
    "* kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类实现形式为 KLDivergence 或 KLD)\n",
    "\n",
    "* cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.自定义损失函数\n",
    "\n",
    "自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n",
    "也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。\n",
    "\n",
    "下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。\n",
    "\n",
    "在类别不平衡和存在难以训练样本的情形下相对于二元交叉熵能够取得更好的效果。\n",
    "\n",
    "详见《如何评价Kaiming的Focal Loss for Dense Object Detection？》\n",
    "\n",
    "https://www.zhihu.com/question/63581984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 1. 1. 1.], shape=(5,), dtype=float32) tf.Tensor([0. 0. 0. 0. 0.], shape=(5,), dtype=float32)\n",
      "tf.Tensor(4.029524, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([1,0,1,1,0],dtype=tf.float32)\n",
    "y_pred = tf.constant([0,0,1,1,0],dtype=tf.float32)\n",
    "pt_1 = tf.where(tf.equal(y_true,1),y_pred,tf.ones_like(y_pred))\n",
    "pt_0 = tf.where(tf.equal(y_true,0),y_pred,tf.zeros_like(y_pred))\n",
    "print(pt_1, pt_0)\n",
    "alpha = 0.25\n",
    "gamma = 2.\n",
    "loss = -tf.reduce_sum(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(1e-07+pt_1)) \\\n",
    "           -tf.reduce_sum((1-alpha) * tf.pow( pt_0, gamma) * tf.math.log(1. - pt_0 + 1e-07))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true,y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true,1),y_pred,tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true,0),y_pred,tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(alpha * tf.pow(1. - pt_1, gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-alpha) * tf.pow( pt_0, gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(losses.Loss):\n",
    "    def __init__(self,gamma=2.,alpha=0.25):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    def call(self,y_pred,y_true):\n",
    "        pt_1 = tf.where(tf.equal(y_true,1),y_pred,tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true,0),y_pred,tf.zeros_like(y_pred))\n",
    "        loss = -tf.reduce_sum(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(1e-07+pt_1)) \\\n",
    "           -tf.reduce_sum((1-alpha) * tf.pow( pt_0, gamma) * tf.math.log(1. - pt_0 + 1e-07))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.评估指标metrics\n",
    "\n",
    "通常损失函数都可以作为评估指标，如MAE,MSE,CategoricalCrossentropy等也是常用的评估指标。\n",
    "\n",
    "但评估指标不一定可以作为损失函数，例如AUC,Accuracy,Precision。因为评估指标不要求连续可导，而损失函数通常要求连续可导。\n",
    "\n",
    "编译模型时，可以通过列表形式指定多个评估指标。\n",
    "\n",
    "如果有需要，也可以自定义评估指标。\n",
    "\n",
    "自定义评估指标需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为评估值。\n",
    "\n",
    "也可以对tf.keras.metrics.Metric进行子类化，重写初始化方法, update_state方法, result方法实现评估指标的计算逻辑，从而得到评估指标的类的实现形式。\n",
    "\n",
    "由于训练的过程通常是分批次训练的，而评估指标要跑完一个epoch才能够得到整体的指标结果。因此，类形式的评估指标更为常见。即需要编写初始化方法以创建与计算指标结果相关的一些中间变量，编写update_state方法在每个batch后更新相关中间变量的状态，编写result方法输出最终指标结果。\n",
    "\n",
    "如果编写函数形式的评估指标，则只能取epoch中各个batch计算的评估指标结果的平均值作为整个epoch上的评估指标结果，这个结果通常会偏离拿整个epoch数据一次计算的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1常用内置评估指标\n",
    "\n",
    "* MeanSquaredError（平方差误差，用于回归，可以简写为MSE，函数形式为mse）\n",
    "\n",
    "* MeanAbsoluteError (绝对值误差，用于回归，可以简写为MAE，函数形式为mae)\n",
    "\n",
    "* MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)\n",
    "\n",
    "* RootMeanSquaredError (均方根误差，用于回归)\n",
    "\n",
    "* Accuracy (准确率，用于分类，可以用字符串\"Accuracy\"表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)\n",
    "\n",
    "* Precision (精确率，用于二分类，Precision = TP/(TP+FP))\n",
    "\n",
    "* Recall (召回率，用于二分类，Recall = TP/(TP+FN))\n",
    "\n",
    "* TruePositives (真正例，用于二分类)\n",
    "\n",
    "* TrueNegatives (真负例，用于二分类)\n",
    "\n",
    "* FalsePositives (假正例，用于二分类)\n",
    "\n",
    "* FalseNegatives (假负例，用于二分类)\n",
    "\n",
    "* AUC(ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)\n",
    "\n",
    "* CategoricalAccuracy（分类准确率，与Accuracy含义相同，要求y_true(label)为onehot编码形式）\n",
    "\n",
    "* SparseCategoricalAccuracy (稀疏分类准确率，与Accuracy含义相同，要求y_true(label)为序号编码形式)\n",
    "\n",
    "* MeanIoU (Intersection-Over-Union，常用于图像分割)\n",
    "\n",
    "* TopKCategoricalAccuracy (多分类TopK准确率，要求y_true(label)为onehot编码形式)\n",
    "\n",
    "* SparseTopKCategoricalAccuracy (稀疏多分类TopK准确率，要求y_true(label)为序号编码形式)\n",
    "\n",
    "* Mean (平均值)\n",
    "\n",
    "* Sum (求和)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2自定义评估指标\n",
    "\n",
    "KS指标适合二分类问题，计算方式为KS= max(TPR-FPR）,TPR曲线实际上就是正样本的累积分布曲线(CDF)，FPR曲线实际上就是负样本的累积分布曲线(CDF).\n",
    "\n",
    "KS指标就是正样本和负样本累积分布曲线的最大值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 函数形式\n",
    "@tf.function\n",
    "def ks(y_true,y_pred):\n",
    "    y_true = tf.reshape(y_true,(-1,))\n",
    "    y_pred = tf.reshape(y_pred,(-1,))\n",
    "    length = tf.shape(y_true)[0]\n",
    "    _,indices = tf.math.top_k(y_pred,length,sorted=False)\n",
    "    y_pred_sorted = tf.gather(y_pred,indices)\n",
    "    y_true_sorted = tf.gather(y_true,indices)\n",
    "    cum_positive_ratio = tf.truediv( ## 正样本有多少个召回了\n",
    "        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted)\n",
    "    )\n",
    "    cum_nagative_ratio = tf.truediv(\n",
    "        tf.cumsum(1- y_true_sorted),tf.reduce_sum(1-y_true_sorted)\n",
    "    )\n",
    "    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio-cum_nagative_ratio))\n",
    "    return ks_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.625, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],[0.7],\n",
    "                      [0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "print(ks(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 类形式自定义评估指标\n",
    "\n",
    "class KS(metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name='ks',**kwargs):\n",
    "        super(KS,self).__init__(name=name,**kwargs)\n",
    "        self.true_positives= self.add_weight(name='tp',shape=(101,),initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp',shape=(101,),initializer='zeros')\n",
    "    \n",
    "    @tf.function\n",
    "    def update_state(self,y_true,y_pred):\n",
    "        y_true = tf.cast(tf.reshape(y_true,(-1,)),tf.bool)\n",
    "        y_pred = tf.cast(100*tf.reshape(y_pred,(-1,)),tf.int32)\n",
    "        \n",
    "        tf.print(y_true, y_pred)\n",
    "        \n",
    "        for i in tf.range(tf.shape(y_true)[0]):\n",
    "            if y_true[i]:\n",
    "                self.true_positives[y_pred[i]].assign(self.true_positives[y_pred[i]] + 1)\n",
    "            else:\n",
    "                self.false_positives[y_pred[i]].assign(self.false_positives[y_pred[i]]+1)\n",
    "        return (self.true_positives,self.false_positives)\n",
    "    \n",
    "    @tf.function\n",
    "    def result(self):\n",
    "        cum_positive_ratio = tf.truediv(\n",
    "            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))\n",
    "        cum_negative_ratio = tf.truediv(\n",
    "            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))\n",
    "        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "        return ks_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.625, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],\n",
    "                      [0.7],[0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "\n",
    "myks = KS()\n",
    "myks.update_state(y_true,y_pred)\n",
    "print(myks.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
